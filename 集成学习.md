随机森林算法梳理

----------
>#####集成学习 
集成学习方法构建多个基学习器，用某种策略将多个结果集成起来，作为最终结果。
基学习器包括：1.  同一学习算法的不同超参数；2.不同假设的不同基础分类器；3. 同一事件的不同表达；4. 不同的训练集 5. 不同的子任务

----------
>#####个体学习器
个体学习器通常是用一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。

----------
>#####boost
通过在训练新模型实例时更注重先前模型错误分类的实例来增量构建集成模型。Boosting方法通过综合较差的学习器(比随机猜效果能好一点的学习器)，来得到一个较强的学习器（较弱学习器的加权和）

--------
>#####结合策略
bagging：使集成模型中的每个模型在组合时具有相同的权重。为了减小模型方差，bagging使用随机抽取的子训练集训练集成中的每个模型，可以视为是一种特殊的模型平均方法，可以降低方差和提高精度
投票(voting)方法：使用所有基础学习器结果的凸组合作为最终决策
Boosting方法：通过综合较差的学习器(比随机猜效果能好一点的学习器)，来得到一个较强的学习器（较弱学习器的加权和）

--------
>#####随机森林思想
随机森林树(Random Forests)算法是一种组合多个树分类器进行分类的方法,其基本思想是每次随机选取一些特征,独立建立树,重复这个过程,保证每次建立树时变量选取的可能性一致,从而得到许多彼此不相关的树,最终的分类结果通过类似Bagging的方法来得到.

--------
>#####优点
    对于很多种数据，它可以产生高准确度的分类器。
    它可以处理大量的输入变量。
    它可以在决定类别时，评估变量的重要性。
    在建造森林时，它可以在内部对于一般化后的误差产生不偏差的估计。
    它包含一个好方法可以估计丢失的数据，并且，如果有很大一部分的数据丢失，仍可以维持准确度。
    它提供一个实验方法，可以去侦测variable interactions。
    对于不平衡的分类数据集来说，它可以平衡误差。
    它计算各例中的亲近度，对于数据挖掘、侦测离群点（outlier）和将数据可视化非常有用。
    使用上述。它可被延伸应用在未标记的数据上，这类数据通常是使用非监督式聚类。也可侦测偏离者和观看数据。
    学习过程是很快速的。

--------
>#####sklearn参数


n_estimators

    定义随机森林中要创建的决策树数量

    通常，越高的值会让预测更强大更稳定，但是过高的值会让训练时间很长

criterion

    定义了分割用的函数

    该函数用来衡量使用每个特征分割的质量从而选择最佳分割

max_features

    定义了每个决策树中可用于分割的最大特征数量

    增加最大特征数通常可以改善性能，但是一个非常高的值会减少各个树之间的差异性

max_depth

    随机森林有多个决策树，此参数定义树的最大深度

min_samples_split

    用于在尝试拆分之前定义叶节点中所需的最小样本数

    如果样本数小于所需数量，则不分割节点

min_samples_leaf

    定义了叶子节点所需的最小样本数

    较小的叶片尺寸使得模型更容易捕获训练数据中的噪声

max_leaf_nodes

    此参数指定每个树的最大叶子节点数

    当叶节点的数量变得等于最大叶节点时，树停止分裂

n_jobs

    这表示并行运行的作业数

    如果要在系统中的所有核心上运行，请将值设置为-1

random_state

    此参数用于定义随机选择


--------
>#####参考
http://staff.ustc.edu.cn/~zwp/teach/MVA/Lec132_slides.pdf
https://zh.wikipedia.org/wiki/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97
https://zh.wikipedia.org/wiki/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0
https://www.jiqizhixin.com/articles/2018-07-28-3

